{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 &ndash; Classification; Exercise 4 &ndash; Spam classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "import tarfile\n",
    "import requests\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = \"datasets\"\n",
    "\n",
    "def fetch_data():\n",
    "    if not os.path.isdir(SPAM_PATH):\n",
    "        os.makedirs(SPAM_PATH)\n",
    "    for filename, url in [('ham.tar.bz2', HAM_URL), ('spam.tar.bz2', SPAM_URL)]:\n",
    "        r = requests.get(url)\n",
    "        filename = os.path.join(SPAM_PATH, filename)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        with tarfile.open(filename) as tf:\n",
    "            tf.extractall(path=SPAM_PATH)\n",
    "            \n",
    "def load_email(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "            \n",
    "def load_data():\n",
    "    HAM_DIR, SPAM_DIR = [os.path.join(SPAM_PATH, dirname) for dirname in ('easy_ham', 'spam')]\n",
    "    if not all([os.path.isdir(dirname) for dirname in [HAM_DIR, SPAM_DIR]]):\n",
    "        print(\"Fetching data.\")\n",
    "        fetch_data()\n",
    "    ham_filenames, spam_filenames = [\n",
    "        [os.path.join(dirname, name) for name in sorted(os.listdir(dirname)) if len(name) > 20]\n",
    "        for dirname in [HAM_DIR, SPAM_DIR]]\n",
    "    return tuple([load_email(fn) for fn in fn_list] for fn_list in (ham_filenames, spam_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails, spam_emails = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def my_train_test_split(mails):\n",
    "    return train_test_split(mails, train_size=0.75, test_size=0.25, random_state=42)\n",
    "ham_train, ham_test = my_train_test_split(ham_emails)\n",
    "spam_train, spam_test = my_train_test_split(spam_emails)\n",
    "\n",
    "X_train = np.array(ham_train + spam_train)\n",
    "X_test = np.array(ham_test + spam_test)\n",
    "y_train = np.array([0] * len(ham_train) + [1] * len(spam_train)) \n",
    "y_test = np.array([0] * len(ham_test) + [1] * len(spam_test))\n",
    "\n",
    "np.random.seed(42)\n",
    "train_shuffle_idxs = np.random.permutation(len(X_train))\n",
    "test_shuffle_idxs = np.random.permutation(len(X_test))\n",
    "X_train = X_train[train_shuffle_idxs]\n",
    "y_train = y_train[train_shuffle_idxs]\n",
    "X_test = X_test[test_shuffle_idxs]\n",
    "y_test = y_test[test_shuffle_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common content_types in Ham:\n",
      "[('text/plain', 1803),\n",
      " ('multipart/signed', 55),\n",
      " ('multipart/mixed', 7),\n",
      " ('multipart/alternative', 7),\n",
      " ('multipart/report', 2)]\n",
      "\n",
      "Most common content_types in Spam:\n",
      "[('text/plain', 158),\n",
      " ('text/html', 139),\n",
      " ('multipart/alternative', 38),\n",
      " ('multipart/mixed', 33),\n",
      " ('multipart/related', 7)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "def count_content_types(mails):\n",
    "    types = Counter()\n",
    "    for m in mails:\n",
    "        ct = m.get_content_type()\n",
    "        types[ct] += 1\n",
    "    return types\n",
    "\n",
    "print('Most common content_types in Ham:')\n",
    "pprint(count_content_types(ham_train).most_common(5))\n",
    "print('\\nMost common content_types in Spam:')\n",
    "pprint(count_content_types(spam_train).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show an example HTML email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Rate Services Conferencing Made Easy Only 18 Cents Per Minute! (Including Long Distance!) No setup fees No contracts or monthly fees Call anytime, from anywhere, to anywhere Connects up to 100 Participants Simplicity in set up and administration Operator Help available 24/7 The Highest Quality Service For The Lowest Rate In The Industry! Fill out the form below to find out how you can lower your phone bill every month. Required Input Field * Name * Web \n",
      "            Address Company \n",
      "            Name * State * Business \n",
      "            Phone * Home \n",
      "            Phone Email \n",
      "            Address * Type of \n",
      "            Business To be removed from this list, send an e-mail to remove@b2b-mail.net Type the word \"remove\" in the subject line. .\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_mails = [mail for mail in X_train if mail.get_content_subtype() == 'html']\n",
    "hm = html_mails[5].get_content()\n",
    "bs = BeautifulSoup(hm, 'html.parser')\n",
    "print(' '.join(list(bs.stripped_strings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "class ContentTypeFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.classes_ = ['PLAIN', 'HTML', 'SIGNED', 'OTHER']\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        def to_class(mail):\n",
    "            ct = mail.get_content_type()\n",
    "            if ct == 'text/plain':\n",
    "                return self.classes_[0]\n",
    "            elif ct == 'text/html':\n",
    "                return self.classes_[1]\n",
    "            elif ct == 'multipart/signed':\n",
    "                return self.classes_[2]\n",
    "            else:\n",
    "                return self.classes_[3]\n",
    "        return np.vectorize(to_class)(x)\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.le_ = LabelEncoder()\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        self.le_.fit(x)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return self.le_.transform(x)\n",
    "    \n",
    "def twod_reshape(X):\n",
    "    return X.reshape(-1, 1)\n",
    "\n",
    "# We can not use lambda, because lambdas are not pickeable\n",
    "one_to_two_reshaper = FunctionTransformer(twod_reshape, validate=False)\n",
    "\n",
    "content_type_pipeline = make_pipeline(ContentTypeFeaturesTransformer(),\n",
    "                                      CategoricalEncoder(),\n",
    "                                      one_to_two_reshaper,\n",
    "                                      OneHotEncoder()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(addr):\n",
    "    m = re.match('.*@(.*)', addr)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return ''\n",
    "\n",
    "class HeaderFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        def get_features(mail):\n",
    "            reply_to = mail.get('Reply-To')\n",
    "            has_reply_to = 1.0 if reply_to else 0.0\n",
    "            from_h = mail.get('From')\n",
    "            reply_to_diff_domain = 0.0\n",
    "            if has_reply_to and from_h:\n",
    "                reply_to_diff_domain = 1.0 if get_domain(reply_to) != get_domain(from_h) else 0.0\n",
    "            has_unsubscribe = 1.0 if mail.get('List-Unsubscribe') else 0.0\n",
    "            return (has_reply_to, reply_to_diff_domain, has_unsubscribe)\n",
    "        return np.vstack(np.vectorize(get_features)(x)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def parse_html(body):\n",
    "    return ' '.join(BeautifulSoup(hm, 'lxml').stripped_strings)\n",
    "\n",
    "def get_content_with_type(mail, include_subject=True):\n",
    "    subject = ''\n",
    "    if include_subject:\n",
    "        subject = ' ' + mail.get('subject')\n",
    "        \n",
    "    body = mail\n",
    "    if mail.is_multipart():\n",
    "        body = mail.get_body(('html', 'plain'))\n",
    "        \n",
    "    try:\n",
    "        if body.get_content_subtype() == 'html':\n",
    "            return ('html', parse_html(body.get_content()) + subject)\n",
    "        elif body.get_content_subtype() == 'plain':\n",
    "            return ('plain', body.get_content() + subject)\n",
    "        else:\n",
    "            return ('other', subject)\n",
    "    except:\n",
    "        return ('other', subject)\n",
    "    \n",
    "def remove_non_alpha(s):\n",
    "    return re.sub('[^a-zA-Z]', '', s)\n",
    "\n",
    "def is_url(s):\n",
    "    return re.match('^http.?://', s)\n",
    "\n",
    "def is_number(s):\n",
    "    return re.match('^[0-9]+$', s)\n",
    "    \n",
    "class MailToWordCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, top_word_count=1000, to_lower=True, parse_url=True, parse_number=True, stem=True,\n",
    "                include_subject=True, per_mail_count=False, only_has_a=False, not_in_dict_col=False):\n",
    "        self.top_word_count = top_word_count\n",
    "        self.to_lower = to_lower\n",
    "        self.parse_url = parse_url\n",
    "        self.parse_number = parse_number\n",
    "        self.stem = stem\n",
    "        self.include_subject = include_subject\n",
    "        self.per_mail_count = per_mail_count\n",
    "        self.only_has_a = only_has_a\n",
    "        self.not_in_dict_col = not_in_dict_col\n",
    "        self.word_counter_ = Counter()\n",
    "        self.mail_counter_ = Counter()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for mail in X:\n",
    "            ct, content = get_content_with_type(mail, self.include_subject)\n",
    "            mail_words = set()\n",
    "            for word in content.split():\n",
    "                word = self._process_word(word)\n",
    "                if len(word):\n",
    "                    self.word_counter_[word] += 1\n",
    "                    if word not in mail_words:\n",
    "                        self.mail_counter_[word] += 1\n",
    "                        mail_words.add(word)\n",
    "        counter = self.mail_counter_ if self.per_mail_count else self.word_counter_\n",
    "        self.top_words_dict_ = dict([(w[0], i) for i, w in enumerate(counter.most_common(self.top_word_count))])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        not_in_dict_shift = 1 if self.not_in_dict_col else 0\n",
    "        feature_count = self.top_word_count + not_in_dict_shift\n",
    "        features = dok_matrix((len(X), feature_count))\n",
    "        for i, mail in enumerate(X):\n",
    "            ct, content = get_content_with_type(mail, self.include_subject)\n",
    "            for word in content.split():\n",
    "                word = self._process_word(word)\n",
    "                if word in self.top_words_dict_:\n",
    "                    col = self.top_words_dict_[word] + not_in_dict_shift\n",
    "                    if self.only_has_a:\n",
    "                        features[i, col] = 1\n",
    "                    else:\n",
    "                        features[i, col] += 1\n",
    "                else:\n",
    "                    if self.only_has_a:\n",
    "                        features[i, 0] = 1\n",
    "                    else:\n",
    "                        features[i, 0] += 1\n",
    "        return features\n",
    "        \n",
    "    \n",
    "    def _process_word(self, word):\n",
    "        if self.parse_url and is_url(word):\n",
    "            return '_URL'\n",
    "        elif self.parse_number and is_number(word):\n",
    "            return '_NUMBER'\n",
    "        else:\n",
    "            return self._alphaword(word)\n",
    "\n",
    "    def _alphaword(self, word):\n",
    "        word = remove_non_alpha(word)\n",
    "        if self.to_lower:\n",
    "            word = word.lower()\n",
    "        if self.stem:\n",
    "            word = stemmer.stem(word)\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.pipeline as pipeline\n",
    "\n",
    "preprocess_pipeline = pipeline.FeatureUnion([\n",
    "    ('content_type', content_type_pipeline),\n",
    "    ('header', HeaderFeaturesTransformer()),\n",
    "    ('word_count', MailToWordCount()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train some initial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV,  RandomizedSearchCV\n",
    "\n",
    "grid_params = {\n",
    "    'prep__word_count__stem': [False, True],\n",
    "    'prep__word_count__top_word_count': [10, 100, 1000, 10000],\n",
    "    'prep__word_count__parse_url': [False, True],\n",
    "    'prep__word_count__parse_number': [False, True],\n",
    "    'prep__word_count__per_mail_count': [False, True],\n",
    "    'prep__word_count__to_lower': [False, True],\n",
    "    'prep__word_count__include_subject': [False, True],\n",
    "    'prep__word_count__not_in_dict_col': [False, True],\n",
    "    'prep__word_count__only_has_a': [False, True],\n",
    "}\n",
    "\n",
    "lr_pipeline = pipeline.Pipeline([\n",
    "    ('prep', preprocess_pipeline),\n",
    "    ('lr', LogisticRegression()),\n",
    "])\n",
    "rs_clf = RandomizedSearchCV(lr_pipeline, n_iter=32, param_distributions=grid_params,\n",
    "                            verbose=True, n_jobs=-1, random_state=42,\n",
    "                            return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed: 10.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('prep', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('content_type', Pipeline(memory=None,\n",
       "     steps=[('contenttypefeaturestransformer', ContentTypeFeaturesTransformer()), ('categoricalencoder', CategoricalEncoder()), ('functiontransformer', FunctionTransformer(accept_sparse=False,\n",
       "    ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=32, n_jobs=-1,\n",
       "          param_distributions={'prep__word_count__include_subject': [False, True], 'prep__word_count__only_has_a': [False, True], 'prep__word_count__parse_number': [False, True], 'prep__word_count__not_in_dict_col': [False, True], 'prep__word_count__parse_url': [False, True], 'prep__word_count__per_mail_count': [False, True], 'prep__word_count__top_word_count': [10, 100, 1000, 10000], 'prep__word_count__to_lower': [False, True], 'prep__word_count__stem': [False, True]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=False, scoring=None, verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_clf.fit(X_train[:1024], y_train[:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display mean test score grouped by each parameter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__include_subject\n",
       "False    0.961487\n",
       "True     0.975525\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__only_has_a\n",
       "False    0.968904\n",
       "True     0.967924\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__parse_number\n",
       "False    0.964600\n",
       "True     0.972412\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__not_in_dict_col\n",
       "False    0.971625\n",
       "True     0.964495\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__parse_url\n",
       "False    0.969727\n",
       "True     0.967671\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__per_mail_count\n",
       "False    0.971875\n",
       "True     0.962891\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__top_word_count\n",
       "10       0.941546\n",
       "100      0.972363\n",
       "1000     0.980835\n",
       "10000    0.975865\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__to_lower\n",
       "False    0.966271\n",
       "True     0.970035\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__stem\n",
       "False    0.970508\n",
       "True     0.966739\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Display mean test score grouped by each parameter.')\n",
    "cv_results = pd.DataFrame.from_dict(rs_clf.cv_results_)\n",
    "for param in grid_params.keys():\n",
    "    display(cv_results.groupby('param_' + param)['mean_test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `~parse_url`, `include_subject`, `~only_has_a`, `stem`, `to_lower`, `top_word_count=1000`, `not_in_dict_col` parameters are the best. Let's try grid search to analyze the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('prep', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('content_type', Pipeline(memory=None,\n",
       "     steps=[('contenttypefeaturestransformer', ContentTypeFeaturesTransformer()), ('categoricalencoder', CategoricalEncoder()), ('functiontransformer', FunctionTransformer(accept_sparse=False,\n",
       "    ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'prep__word_count__per_mail_count': [False, True], 'prep__word_count__parse_number': [False, True]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipeline.set_params(\n",
    "    prep__word_count__parse_url=False,\n",
    "    prep__word_count__include_subject=True,\n",
    "    prep__word_count__only_has_a=False,\n",
    "    prep__word_count__stem=True,\n",
    "    prep__word_count__to_lower=True,\n",
    "    prep__word_count__top_word_count=1000,\n",
    "    prep__word_count__not_in_dict_col=True,\n",
    ")\n",
    "\n",
    "grid_params = {\n",
    "    'prep__word_count__parse_number': [False, True],\n",
    "    'prep__word_count__per_mail_count': [False, True],\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(lr_pipeline, param_grid=grid_params,\n",
    "                    verbose=True, n_jobs=-1, return_train_score=False)\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display mean test score grouped by each parameter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__per_mail_count\n",
       "False    0.986222\n",
       "True     0.984667\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "param_prep__word_count__parse_number\n",
       "False    0.985556\n",
       "True     0.985333\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Display mean test score grouped by each parameter.')\n",
    "cv_results = pd.DataFrame.from_dict(gs_clf.cv_results_)\n",
    "for param in grid_params.keys():\n",
    "    display(cv_results.groupby('param_' + param)['mean_test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `~per_mail_count` and `parse_number`. And that gives us ~98.8% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('prep', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('content_type', Pipeline(memory=None,\n",
       "     steps=[('contenttypefeaturestransformer', ContentTypeFeaturesTransformer()), ('categoricalencoder', CategoricalEncoder()), ('functiontransformer', FunctionTransformer(accept_sparse=False,\n",
       "    ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipeline.set_params(\n",
    "    prep__word_count__parse_number=False,\n",
    "    prep__word_count__per_mail_count=True,\n",
    ")\n",
    "lr_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the generalization accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9826666666666667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, lr_pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.3% accuracy on the test set. Not bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
